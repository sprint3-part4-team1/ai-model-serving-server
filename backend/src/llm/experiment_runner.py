"""
LLM ì‹¤í—˜ ëŸ¬ë„ˆ
ë™ì¼ í”„ë¡¬í”„íŠ¸ë¡œ ì—¬ëŸ¬ ëª¨ë¸ ì‹¤í—˜ ë° ë¹„êµ ë¶„ì„
"""

import time
import json
from typing import Dict, List, Any, Optional
from datetime import datetime

from .gpt5_provider import GPT5Provider
from .gpt4_provider import GPT4Provider
from .gemini_provider import GeminiProvider
from .base_provider import BaseLLMProvider


class ExperimentRunner:
    """
    LLM ì‹¤í—˜ ëŸ¬ë„ˆ
    - ë™ì¼ í”„ë¡¬í”„íŠ¸ë¡œ ì—¬ëŸ¬ ëª¨ë¸ ì‹¤í—˜
    - ì‘ë‹µ í’ˆì§ˆ, ì†ë„, ë¹„ìš© ë¹„êµ
    - ê²°ê³¼ ì €ì¥ ë° ë¶„ì„
    """
    
    def __init__(self):
        self.providers = {
            "gpt-5.1": GPT5Provider(),
            "gpt-4.1": GPT4Provider(),
            "gemini-2.5-flash": GeminiProvider()
        }
        self.experiments = []
    
    def run_experiment(
        self, 
        prompt: str, 
        models: Optional[List[str]] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        ì—¬ëŸ¬ ëª¨ë¸ë¡œ ë™ì¼ í”„ë¡¬í”„íŠ¸ ì‹¤í–‰
        
        Args:
            prompt (str): ì‹¤í—˜í•  í”„ë¡¬í”„íŠ¸
            models (list): ì‹¤í—˜í•  ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ì „ì²´)
            **kwargs: ëª¨ë¸ë³„ ì¶”ê°€ íŒŒë¼ë¯¸í„°
        
        Returns:
            dict: {
                "experiment_id": "exp_20251127_001",
                "prompt": "...",
                "timestamp": "2025-11-27T11:00:00",
                "results": {
                    "gpt-5-mini": {...},
                    "gpt-4o": {...},
                    "gemini-2.5-flash": {...}
                }
            }
        """
        # ì‹¤í—˜ ID ìƒì„±
        exp_id = f"exp_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        print("=" * 80)
        print(f"ğŸ”¬ ì‹¤í—˜ ì‹œì‘: {exp_id}")
        print("=" * 80)
        print(f"í”„ë¡¬í”„íŠ¸: {prompt[:100]}...")
        print()
        
        # ì‹¤í—˜í•  ëª¨ë¸ ì„ íƒ
        target_models = models or list(self.providers.keys())
        
        results = {}
        
        for model_name in target_models:
            if model_name not in self.providers:
                print(f"âš ï¸ {model_name}: ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸")
                continue
            
            provider = self.providers[model_name]
            
            print(f"ğŸš€ {model_name} ì‹¤í–‰ ì¤‘...")
            
            # âœ… ì´ˆê¸°í™”!
            json_parsable = False
            parsed_data = None

            try:
                # ì‘ë‹µ ìƒì„±
                start_time = time.time()
                response = provider.create_response(prompt, **kwargs)
                elapsed_time = time.time() - start_time
                
                # í† í° ìˆ˜ ì¶”ì • (ê°„ë‹¨í•œ ê³„ì‚°)
                input_tokens = len(prompt.split()) * 1.3  # ëŒ€ëµì  ì¶”ì •
                output_tokens = len(response.split()) * 1.3
                
                # ë¹„ìš© ê³„ì‚°
                cost_info = provider.get_cost_per_1k_tokens()
                estimated_cost = (
                    (input_tokens / 1000) * cost_info['input'] +
                    (output_tokens / 1000) * cost_info['output']
                )
                
                try:
                    parsed_data = provider.parse_json_response(response)
                    json_parsable = True
                    print(f"  âœ“ JSON íŒŒì‹± ì„±ê³µ")
                except Exception as parse_error:
                    json_parsable = False
                    print(f"  âœ— JSON íŒŒì‹± ì‹¤íŒ¨: {str(parse_error)[:50]}")

                results[model_name] = {
                    "success": True,
                    "response": response,
                    "elapsed_time": elapsed_time,
                    "input_tokens": int(input_tokens),
                    "output_tokens": int(output_tokens),
                    "estimated_cost": estimated_cost,
                    "json_parsable": json_parsable,
                    "parsed_data": parsed_data,
                    "response_length": len(response)
                }
                
                print(f"  âœ… ì„±ê³µ ({elapsed_time:.2f}s, ${estimated_cost:.6f})")
                print(f"  ğŸ“Š í† í°: {int(input_tokens)} in / {int(output_tokens)} out")
                print(f"  ğŸ“ ì‘ë‹µ ê¸¸ì´: {len(response)} chars")
                print()
            
            except Exception as e:
                results[model_name] = {
                    "success": False,
                    "error": str(e),
                    "elapsed_time": 0,
                    "estimated_cost": 0,
                    "json_parsable": False
                }
                print(f"  âŒ ì‹¤íŒ¨: {e}")
                print()
        
        # ì‹¤í—˜ ê²°ê³¼ ì €ì¥
        experiment_data = {
            "experiment_id": exp_id,
            "prompt": prompt,
            "timestamp": datetime.now().isoformat(),
            "models_tested": target_models,
            "results": results
        }
        
        self.experiments.append(experiment_data)
        
        print("=" * 80)
        print("âœ… ì‹¤í—˜ ì™„ë£Œ")
        print("=" * 80)
        
        return experiment_data
    
    def compare_results(self, experiment_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        ì‹¤í—˜ ê²°ê³¼ ë¹„êµ ë¶„ì„
        
        Args:
            experiment_data: run_experiment()ì˜ ë°˜í™˜ê°’
        
        Returns:
            dict: ë¹„êµ ë¶„ì„ ê²°ê³¼
        """
        results = experiment_data['results']
        
        # ì„±ê³µí•œ ëª¨ë¸ë§Œ ì¶”ì¶œ
        successful = {
            model: data for model, data in results.items() 
            if data.get('success', False)
        }
        
        if not successful:
            return {"error": "ì„±ê³µí•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤"}
        
        # ìˆœìœ„ ê³„ì‚°
        rankings = {
            "speed": sorted(successful.items(), key=lambda x: x[1]['elapsed_time']),
            "cost": sorted(successful.items(), key=lambda x: x[1]['estimated_cost']),
            "response_length": sorted(successful.items(), key=lambda x: x[1]['response_length'], reverse=True)
        }
        
        # JSON ì„±ê³µ ê°œìˆ˜ ì •í™•í•˜ê²Œ ê³„ì‚°
        json_success_count = sum(1 for m in successful.values() if m.get('json_parsable', False))
        json_success_rate = json_success_count / len(successful) if successful else 0

        # í†µê³„ ê³„ì‚°
        comparison = {
            "fastest_model": rankings['speed'][0][0],
            "fastest_time": rankings['speed'][0][1]['elapsed_time'],
            "cheapest_model": rankings['cost'][0][0],
            "cheapest_cost": rankings['cost'][0][1]['estimated_cost'],
            "most_detailed": rankings['response_length'][0][0],
            "json_success_rate": json_success_rate,
            "json_success_count": json_success_count, 
            "rankings": {
                "speed": [(m, d['elapsed_time']) for m, d in rankings['speed']],
                "cost": [(m, d['estimated_cost']) for m, d in rankings['cost']]
            },
            "total_models_tested": len(results),
            "successful_models": len(successful)
        }
        
        return comparison
    
    def print_comparison(self, comparison: Dict[str, Any]):
        """ë¹„êµ ê²°ê³¼ë¥¼ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥"""
        print("\n" + "=" * 80)
        print("ğŸ“Š ë¹„êµ ë¶„ì„ ê²°ê³¼")
        print("=" * 80)
        
        print(f"\nğŸ† ê°€ì¥ ë¹ ë¥¸ ëª¨ë¸: {comparison['fastest_model']} ({comparison['fastest_time']:.2f}s)")
        print(f"ğŸ’° ê°€ì¥ ì €ë ´í•œ ëª¨ë¸: {comparison['cheapest_model']} (${comparison['cheapest_cost']:.6f})")
        print(f"ğŸ“ ê°€ì¥ ìƒì„¸í•œ ì‘ë‹µ: {comparison['most_detailed']}")
        print(f"âœ“ JSON íŒŒì‹± ì„±ê³µë¥ : {comparison['json_success_rate']*100:.1f}%")
        
        print("\nğŸ“ˆ ì†ë„ ìˆœìœ„:")
        for i, (model, time) in enumerate(comparison['rankings']['speed'], 1):
            print(f"  {i}. {model}: {time:.2f}s")
        
        print("\nğŸ’µ ë¹„ìš© ìˆœìœ„:")
        for i, (model, cost) in enumerate(comparison['rankings']['cost'], 1):
            print(f"  {i}. {model}: ${cost:.6f}")
        
        print("\n" + "=" * 80)
    
    def run_batch_experiments(
        self, 
        prompts: List[str], 
        models: Optional[List[str]] = None
    ) -> List[Dict[str, Any]]:
        """
        ì—¬ëŸ¬ í”„ë¡¬í”„íŠ¸ë¡œ ë°°ì¹˜ ì‹¤í—˜
        
        Args:
            prompts (list): ì‹¤í—˜í•  í”„ë¡¬í”„íŠ¸ ë¦¬ìŠ¤íŠ¸
            models (list): ì‹¤í—˜í•  ëª¨ë¸ ë¦¬ìŠ¤íŠ¸
        
        Returns:
            list: ê° ì‹¤í—˜ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        print(f"\nğŸ”¬ ë°°ì¹˜ ì‹¤í—˜ ì‹œì‘: {len(prompts)}ê°œ í”„ë¡¬í”„íŠ¸")
        print("=" * 80)
        
        batch_results = []
        
        for i, prompt in enumerate(prompts, 1):
            print(f"\n[{i}/{len(prompts)}] í”„ë¡¬í”„íŠ¸ ì‹¤í—˜ ì¤‘...")
            result = self.run_experiment(prompt, models)
            batch_results.append(result)
        
        print("\nâœ… ë°°ì¹˜ ì‹¤í—˜ ì™„ë£Œ")
        return batch_results
    
    def get_aggregate_statistics(self) -> Dict[str, Any]:
        """
        ì „ì²´ ì‹¤í—˜ì˜ í†µí•© í†µê³„
        
        Returns:
            dict: ëª¨ë¸ë³„ í‰ê·  ì„±ëŠ¥
        """
        if not self.experiments:
            return {"error": "ì‹¤í—˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"}
        
        stats = {}
        
        for exp in self.experiments:
            for model, result in exp['results'].items():
                if not result.get('success', False):
                    continue
                
                if model not in stats:
                    stats[model] = {
                        "total_calls": 0,
                        "total_time": 0,
                        "total_cost": 0,
                        "json_success": 0
                    }
                
                stats[model]['total_calls'] += 1
                stats[model]['total_time'] += result['elapsed_time']
                stats[model]['total_cost'] += result['estimated_cost']

                if result.get('json_parsable', False) is True:
                    stats[model]['json_success'] += 1
            
        # í‰ê·  ê³„ì‚°
        aggregate = {}
        for model, data in stats.items():
            calls = data['total_calls']
            aggregate[model] = {
                "total_calls": calls,
                "avg_time": data['total_time'] / calls,
                "avg_cost": data['total_cost'] / calls,
                "json_success_rate": data['json_success'] / calls,
                "json_success_count": data['json_success'] 
            }
        
        return aggregate
    
    def save_experiments(self, filepath: str = "experiments.json"):
        """ì‹¤í—˜ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        export_data = {
            "total_experiments": len(self.experiments),
            "experiments": self.experiments,
            "aggregate_statistics": self.get_aggregate_statistics()
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ ì‹¤í—˜ ê²°ê³¼ ì €ì¥: {filepath}")
    
    def generate_report(self, filepath: str = "experiment_report.md"):
        """
        ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ìƒì„±
        """
        stats = self.get_aggregate_statistics()
        
        if "error" in stats:
            print("âš ï¸ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: ì‹¤í—˜ ë°ì´í„° ì—†ìŒ")
            return
        
        report = f"""# LLM ì‹¤í—˜ ë¦¬í¬íŠ¸

ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
ì´ ì‹¤í—˜ ìˆ˜: {len(self.experiments)}

## ğŸ“Š ëª¨ë¸ë³„ í‰ê·  ì„±ëŠ¥

| ëª¨ë¸ | í˜¸ì¶œ ìˆ˜ | í‰ê·  ì†ë„ | í‰ê·  ë¹„ìš© | JSON ì„±ê³µë¥  |
|------|---------|-----------|-----------|-------------|
"""
        
        for model, data in stats.items():
            report += f"| {model} | {data['total_calls']} | {data['avg_time']:.2f}s | ${data['avg_cost']:.6f} | {data['json_success_rate']*100:.1f}% |\n"
        
        report += "\n## ğŸ† ì¢…í•© í‰ê°€\n\n"
        
        # ìµœê³  ëª¨ë¸ ì„ ì •
        fastest = min(stats.items(), key=lambda x: x[1]['avg_time'])
        cheapest = min(stats.items(), key=lambda x: x[1]['avg_cost'])
        most_reliable = max(stats.items(), key=lambda x: x[1]['json_success_rate'])
        
        report += f"- **ê°€ì¥ ë¹ ë¥¸ ëª¨ë¸**: {fastest[0]} ({fastest[1]['avg_time']:.2f}s)\n"
        report += f"- **ê°€ì¥ ì €ë ´í•œ ëª¨ë¸**: {cheapest[0]} (${cheapest[1]['avg_cost']:.6f})\n"
        report += f"- **ê°€ì¥ ì•ˆì •ì ì¸ ëª¨ë¸**: {most_reliable[0]} ({most_reliable[1]['json_success_rate']*100:.1f}% ì„±ê³µë¥ )\n"
        
        report += "\n## ğŸ“ˆ ì‹¤í—˜ ìƒì„¸\n\n"
        
        for i, exp in enumerate(self.experiments, 1):
            report += f"### ì‹¤í—˜ {i}: {exp['experiment_id']}\n\n"
            report += f"**í”„ë¡¬í”„íŠ¸**: {exp['prompt'][:100]}...\n\n"
            
            report += "| ëª¨ë¸ | ì„±ê³µ | ì‹œê°„ | ë¹„ìš© |\n"
            report += "|------|------|------|------|\n"
            
            for model, result in exp['results'].items():
                if result.get('success'):
                    report += f"| {model} | âœ… | {result['elapsed_time']:.2f}s | ${result['estimated_cost']:.6f} |\n"
                else:
                    report += f"| {model} | âŒ | - | - |\n"
            
            report += "\n"
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"ğŸ“„ ë¦¬í¬íŠ¸ ìƒì„±: {filepath}")


# ì‹±ê¸€í†¤
_experiment_runner = None

def get_experiment_runner():
    """ì‹¤í—˜ ëŸ¬ë„ˆ ì‹±ê¸€í†¤ ë°˜í™˜"""
    global _experiment_runner
    if _experiment_runner is None:
        _experiment_runner = ExperimentRunner()
    return _experiment_runner
